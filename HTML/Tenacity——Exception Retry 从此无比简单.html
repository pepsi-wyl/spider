 
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
    </head>
    <body>
    <p class="site-subtitle">给时光以生命。</p> <p><a href="https://kingname.info/2017/04/17/decorate-for-method/" rel="noopener" target="_blank">Python 装饰器装饰类中的方法</a>这篇文章，使用了装饰器来捕获代码异常。这种方式可以让代码变得更加简洁和Pythonic。</p> <p>在写代码的过程中，处理异常并重试是一个非常常见的需求。但是如何把捕获异常并重试写得简洁高效，这就是一个技术活了。</p> <p>以爬虫开发为例，由于网页返回的源代码有各种不同的情况，因此捕获异常并重试是很常见的要求。下面这几段代码是我多年以前，在刚开始学习爬虫的时候，由于捕获异常并重试导致代码混乱化过程。</p> <p>代码一开始的逻辑非常简单，获取网页后台API返回的JSON字符串，转化成字典，提取出里面<code>data</code>的数据，然后传递给<code>save()</code>函数:</p> <p>代码运行一段时间，发现有时候JSON会随机出现解析错误。于是添加捕获异常并重试的功能：</p> <p>后来又发现，有部份的URL会导致递归深度超过最大值。这是因为有一些URL返回的是数据始终是错误的，而有些URL，重试几次又能返回正常的JSON数据，于是限制只重试3次：</p> <p>后来又发现，不能立刻重试，重试要有时间间隔，并且时间间隔逐次增大……</p> <p>从上面的例子中可以看到，对于异常的捕获和处理，一不小心就让整个代码变得很难看很难维护。为了解决这个问题，就需要通过装饰器来完成处理异常并重试的功能。</p> <p>Python 有一个第三方库，叫做<a href="https://tenacity.readthedocs.io/en/latest/" rel="noopener" target="_blank">Tenacity</a>，它实现了一种优雅的重试功能。</p> <p>以上面爬虫最初的无限重试版本为例，如果想实现遇到异常就重试。只需要添加两行代码，爬虫的主体函数完全不需要做修改：</p> <p>现在要限制重试次数为3次，代码总行数不需要新增一行就能实现：</p> <p>现在想每5秒钟重试一次，代码行数也不需要增加：</p> <p>甚至重试的时间间隔想指数级递增，代码行数也不需要增加：<br/><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tenacity <span class="keyword">import</span> retry wait_exponential</span><br/><span class="line"></span><br/><span class="line"><span class="meta">@retry(wait=wait_exponential(multiplier=1 max=10)) # 重试时间间隔满足：2^n * multiplier n为重试次数，但最多间隔10秒</span></span><br/><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(url)</span>:</span></span><br/><span class="line">    info_json = requests.get(url).content.decode()</span><br/><span class="line">    info_dict = json.loads(info_json)</span><br/><span class="line">    data = info_dict[<span class="string">'data'</span>]</span><br/><span class="line">    save(data)</span><br/></pre></td></tr></table></figure></p> <p>重试不仅可以限制次数和间隔时间，还可以针对特定的异常进行重试。在爬虫主体中，其实有三个地方可能出现异常: </p> <p>如果只需要在JSON解析错误时重试，由于异常类型为<code>json.decoder.JSONDecodeError</code>，所以就可以通过参数来进行限制：</p> <p>当然，这些特性都可以进行组合，例如只对<code>JSONDecodeError</code> 进行重试，每次间隔5秒，重试三次，那就写成：</p> <p>自始至终，爬虫主体的代码完全不需要做任何修改。</p> <p>Tenacity是我见过的，最 Pythonic ，最优雅的第三方库。</p> <p class="site-author-name" itemprop="name">谢乾坤 | Kingname</p> <p class="site-description motion-element" itemprop="description">高级数据挖掘工程师，《Python 爬虫开发 从入门到实战》、《左手 MongoDB 右手 Redis——从入门到商业实战》作者。 微软最有价值专家 MVP，Python Scrapy MongoDB Redis Pandas Golang。</p>
    </body>
    </html>
    